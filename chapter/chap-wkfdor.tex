\chapter{基于半监督学习的序回归技术}
\label{chap:wkfdor}

%谈一下半监督序回归的motivation

\section{半监督序回归问题定义}
%和有监督序回归问题的定义类似
考虑一个有\(K\)个有序类别的半监督序回归问题，用一个连续的整数集合 \(Z=\{1,2,\dots,K\}\)来表示，整数的值表示相应类别在整个标签集合中的序。在半监督序回归问题中，给定一个包含\(L\)个有标签样例\((X^{l},Y^{l})=\{(x_{1:L},y_{1:L})\}\)和\(N-L\)个无标签样例\(X^{u} = {x_{L+1:N}}\)的训练集合，其中\(x_{i} \in \chi \subseteq R^{d}\)，\(y_{i} \in Z\)。我们的目标是构造一个半监督序回归模型，通过用训练集来学习，使得该模型可以对测试样例给出一个预测的标签。

\section{半监督学习}
在机器学习中，半监督学习通过在训练的过程中不仅使用有标签数据，同时还使用无标签数据，从而获得比只使用有标签数据训练更好的性能。为了使用无标签数据，需要对数据的分布做一个假设，称为平滑假设（smoothness assumption）\citep{chapelle2006semi}。
%[O.Chapelle,B.Scho ̈lkopf,andA.Zien,Semi-SupervisedLearn-ing. MIT Press Cambridge, 2006.]。
基于这个假设，出现了多种半监督学习方法，例如基于图的方法（graph-based methods）、低密度分离方法（low-density separation methods）、生成模型（generative models）等\citep{zhu2005semi}。
%[X. Zhu, “Semi-supervised learning literature survey,” Technical Report 1530, Department of Computer Sciences, University of Wisconsin, Madison, Tech. Rep., 2006.]。

\section{KDLOR算法回顾}
基于核判别分析的序回归技术（KDLOR\citep{sun2010kernel}）是一种高效的有监督序回归技术。KDLOR通过使用一个序约束来使用序信息，将传统的核判别分析（KFD）方法拓展到序回归问题。核判别分析是一种高效的降维和分类方法，它通过最大化一个函数来得到一个投影向量，使得同类数据在投影后的空间里能够很紧凑，同时不同类别的数据在投影后的空间里能够分散开来，从而最小化类别之间在重叠。原始数据通过投影之后，可以轻松地被分类。核判别分析通过计算类中心、类内协方差矩阵、类间协方差矩阵来有效地表示数据分布。由于KDLOR是以核判别分析为基本框架，所以它也可以充分地利用数据分布的信息。而基于支持向量机的序回归方法由于是通过支持向量来决定分类面，所以会忽略数据的全局信息，从而可能导致得到的解不合理\citep{sun2010kernel}。相比较于一些流行的有监督序回归方法，KDLOR不仅可以获得很有竞争力的分类性能同时也有相对最低的计算复杂度。

KDLOR的基本任务是找到一个可以保存序信息的最佳投影向量。为了保证序的正确性，在映射后的空间，序更高的类的均值要大于序更低的类。此外，为了达到FDA的目标，该投影向量必须保证相邻序的类别之间有大的间隔，同时每个类别内部的数据能够有小的方差。

KDLOR模型在线性情况下的数学形式表示如下：
\begin{equation}
\label{kdlor}
\left\{\begin{array}{rll} \min & J(w,\rho)=w^{T}\cdot S_{w}\cdot w-C\cdot \rho& \\
\\
s.t. & w^{T}\cdot (m_{k+1}-m_{k})\geq\rho,{} \\
     & k = 1,2,\dots,K-1\\
\end{array}
\right.
\end{equation}
其中 \(S_{w} = \frac{1}{L}\sum_{k=1}^{K}\sum_{x\in X_{k}}(x-m_{k})(x-m_{k})^{T}\) 是类间协方差矩阵（within-class covariance matrix），\(m_{k}=\frac{1}{L_{k}}\sum_{x\in X_{k}}x\) 是第 \(k\)个类别的样例的均值，\(L\) 是有标签样例的个数，\(X_{k}\) 是第\(k\)个类别的样例集合。该优化问题的目标是找到一个投影向量 \(w\) ，可以满足上文提到的两个要求。惩罚系数 \(C\)用来平衡目标函数中的两项。如果\(\rho>0\)，就可以保证序关系的正确性。非线性情况下模型的形式和上面类似，只需要使用一定的核技巧（kernel trick）就可以得到。

由于KDLOR是以FDA为基本框架，所以它也可以充分地利用数据分布的信息。而基于支持向量机的序回归方法由于是通过支持向量来决定分类面，所以会忽略数据的全局信息，从而可能导致得到的解不合理[7]。此外，相比较于一些流行的有监督序回归方法，KDLOR不仅可以获得很有竞争力的分类性能同时也有相对最低的计算复杂度。
%Besides, KDLOR has the smallest problem size (i.e., at most L [7]) in comparison to the ap- proaches in [1–5]. 
但是，当有标记数据很少的时候，有监督序回归方法就会受到限制。对于KDLOR来说，这个限制就是类的中心趋势难以准确地估计。

\section{基于加权核判别分析的半监督序回归技术}
对于基于FDA的方法，用训练数据估计类均值和协方差矩阵的准确性将对序回归模型的性能产生重要的影响。而当有标签数据很少的时候，仅用有标签数据很难准确地估计类分布。因此，我们考虑利用存在的无标签数据来更准确地估计类分布。本节我们提出了一种半监督核判别分析序回归技术。具体的说，我们首先估计无标签数据对于每个类别的隶属度（membership），其次通过一个加权策略来计算整个训练集（包括有标签数据和无标签数据）上的类分布，再将这个加权类分布应用到KDLOR框架下得到最终模型。下面先介绍计算无标签数据隶属度的方法，再介绍我们提出的加权核判别分析半监督序回归技术（Weighted Kernel Fisher Discriminant Analysis for Ordinal Regression, WKFDOR）。

\subsection{估计无标签数据对每个类别的隶属度}
这里我们使用一个标签传播（label propagation）的方法（the consistency method）\citep{zhou2004learning}
%[D. Zhou, O. Bousquet, T. N. Lal, J. Weston, and B. Scho ̈lkopf,“Learning with local and global consistency,” in Advances in Neural Information Processing Systems 16. MIT Press, 2004, pp. 321–328.]）
来估计无标签数据对于每个类别的隶属度。\autoref{alg_mem}列出了\textit{mConsistency}算法的主要步骤。

\IncMargin{1em}
\begin{algorithm}
\SetKwData{Left}{left}\SetKwData{This}{this}\SetKwData{Up}{up}
\SetKwFunction{Union}{Union}\SetKwFunction{FindCompress}{FindCompress}
\SetKwInOut{Input}{input}\SetKwInOut{Output}{output}
\Input{有标签数据 $(X^{l},Y^{l})=\{(x_{1:L},y_{1:L})\}$; \\
 无标签数据$X^{u}=\{(x_{L+1:N})\}$}
\Output{隶属度矩阵 $U=(u_{jk})_{N \times K}$}
\BlankLine
\emph{$Y_{N \times K}$ $\leftarrow$ $Y_{jk}=\begin{cases}1,\; y_{j}=k \\0,\; otherwise\end{cases}$}\;
\emph{$W_{N \times N}$ $\leftarrow$ $W_{ij}=\begin{cases}\exp(-\|x_{i}-x_{j}\|^{2}/2\varepsilon^{2}),\; i\neq j \\0,\; otherwise\end{cases}$}\;
\emph{$S=D^{-1/2}WD^{-1/2}$，$D$是一个对角矩阵，$D_{ii}=\sum_{j=1}^{N}W_{ij}$}\;
\emph{$F^{*}=(1-\alpha)(I-\alpha S)^{-1}Y$, $\alpha\in(0,1)$}\;
\emph{$F^{n}_{N \times K}$ $\leftarrow$ $F^{n}_{jk}=F^{*}_{jk}/\sum_{k=1}^{K}F^{*}_{jk}$}\;
\emph{$U$ $\leftarrow$ $u_{jk}=\begin{cases}Y_{jk},\; x_{j}\in X^{l} \\F^{n}_{jk},\; x_{j}\in X^{u}\end{cases}$}\;
\caption{mConsistency}\label{alg_mem}
\end{algorithm}
\DecMargin{1em}

该算法的主要思想是，通过让每一个数据点迭代地传播它的标签信息给它的邻近点，直到达到全局稳定状态\citep{zhou2004learning}。其形式化表示是\(F(t+1)=\alpha SF(t)+(1-\alpha)Y\)，参数\(\alpha\)用来平衡标签信息的扩散（第一项）和原始状态的维持（第二项），\(Y\)是初始标签矩阵，\(S\)是对邻近度矩阵（affinity matrix）\(W\)的对称归一化。迭代过程会收敛到\(F^{*}=(1-\alpha)(I-\alpha S)^{-1}Y\)\citep{zhou2004learning}，所以标签传播的最终状态可以直接通过\(F^{*}\)来计算。由于标签信息是通过数据点之间的邻近度矩阵来扩散的，所以该算法可以被看作是一种基于图的半监督学习方法（graph-based semi-supervised method）。对每个无标签数据点\(x_{i}\)，它最终接收到的标签信息存储在一个行向量（row vector）\(F^{*}_{i}=[F^{*}_{i1},\dots,F^{*}_{iK}]\)里。根据平滑假设，\(F^{*}_{i}\)可以表示\(x_{i}\)属于每个类别的置信度（confidence）。因此，我们归一化\(F^{*}\)并把它作为隶属度矩阵，表示为 \(U=(u_{jk})_{N \times K}\)。这里需要指出的是，有标签数据的隶属度直接从给定的标签\(Y^{l}\)得到，具体请参\autoref{alg_mem}中的步骤6。

\textit{mConsistency}算法简单且快速，它的主要计算开销仅仅花在一个矩阵求逆和一个矩阵乘法操作上，时间复杂度是\(O(N^{3})\)，其中\(N\)是训练样例的个数。

\subsection{基于加权核判别分析的半监督序回归技术}
为了利用无标签数据来提升对类分布估计的准确性，我们使用一个加权策略来计算类均值和类内协方差矩阵。考虑到实际应用中的数据往往在原始空间是非线性的，所以我们在这里使用了核技术（kernel method）。具体的做法是：首先通过一个非线性映射函数 \(\phi\)，将原始数据映射到一个更高维度的空间 \(\mathcal{F}\)，使得数据在该空间是线性可分的；接着类似于线性情况下的做法，在映射后的数据上求得最优的投影向量。将原始空间映射到 \(\mathcal{F}\)之后，加权类均值（weighted class means）和加权类内协方差矩阵（weighted within-class covariance matrix）定义如下：
\begin{gather}
\label{weighted_mean}
m^{\phi}_{k}=\frac{\sum_{j=1}^{N} u_{jk}\phi(x_{j})}{\sum_{j=1}^{N} u_{jk}} \\
\label{weighted_cov}
S^{\phi}_{w}=\frac{1}{u} \sum_{k=1}^{K} \sum_{j=1}^{N} u_{jk}(\phi(x_{j})-m^{\phi}_{k})(\phi(x_{j})-m^{\phi}_{k})^{T}
\end{gather}
其中所有的训练数据（包括有标签和无标签样例）都被用来顾及类分布。 \(u=\sum_{k=1}^{K} \sum_{j=1}^{N} u_{jk}\)，\(u_{jk}\)是 \(x_{j}\)属于第\(k\)个类别的隶属度。通过公式\autoref{weighted_mean}和\autoref{weighted_cov}可以看到，那些有对某个类别有更高隶属度的数据点将会对该类的类均值和类内协方差矩阵贡献更大，而这两个类分布的特征分别反映了类的中心趋势和类的分布形状。假设对所有无标签数据的隶属度的估计都完全正确的话（即当\(x_{j}\)的真实标签是\(k\)时, \(u_{jk}=1\) 而\(u_{jk^{c}}=0,\;k^{c}\neq k\)），通过该加权策略，相当于额外使用了\(N-L\)个有标签数据来计算类分布，这毫无疑问会比仅使用少量的有标签数据更加准确。

%翻译 lie in the span of
下面，我们将详细推导基于加权核判别分析的半监督序回归（WKFDOR）模型的数学形式。根据再生核理论（reproducing kernels theory），任何解 \(w\in \mathcal{F}\)必须要满足如下形式：
%在所有在(\mathcal{F}\)中的训练样例形成的
\begin{equation}
\label{kernel_w}
w=\sum_{i=1}^{N}\alpha_{i}\phi(x_{i}),\quad \alpha_{i} \in R
\end{equation}
其中 \(\alpha_{i}\) 是第\(i\)个样例的系数。通过投影向量\(w\)，在映射后的空间里，加权类均值和加权类内协方差矩阵由下面公式计算：
\begin{gather}
\label{project_mean}
w^{T} \cdot m^{\phi}_{k}=\alpha^{T} \cdot \mathcal{M}_{k} \\
\label{project_cov}
w^{T} \cdot S^{\phi}_{w} \cdot w=\alpha^{T} \cdot \mathcal{N} \cdot \alpha
\end{gather}
其中\(\alpha=[\alpha_{1},\alpha_{2},\dots,\alpha_{N}]^{T}\)。\(\mathcal{M}_{k}\)是一个\(N \times 1\)的列向量，其中
\begin{equation}
\label{M}
(\mathcal{M}_{k})_{i}=\frac{1}{u_{k}}\sum_{j=1}^{N}u_{jk}  k(x_{i},x_{j})
\end{equation}
\(\mathcal{N}\) 是一个\(N\times N\) 的矩阵，如下计算：
\begin{equation}
\label{N}
\mathcal{N}=\frac{1}{u}\mathcal{K}\cdot[\sum_{k=1}^{K}(D_{k}-D_{k} \cdot \mathbf{1}_{u_{k}} \cdot D_{k})]\cdot \mathcal{K}
\end{equation}
其中\(\mathcal{K}=(k(x_{i},x_{j}))_{N \times N}\)表示核矩阵（kernel matrix），\(k(\cdot,\cdot)\)是核函数（kernel function）。\(D_{k}\)是一个对角矩阵，对角线元素 \((D_{k})_{ii}=u_{ik}\)。\(\mathbf{1}_{u_{k}}\) 是一个所有项都等于\(1/u_k\)的矩阵，其中\(u_{k}=\sum_{j=1}^{N}u_{jk}\)。

基于加权核判别分析的半监督序回归算法（WKFDOR）的目标是找到一个最优的投影向量，使得在映射后的空间里，同类数据点能够相聚紧凑，并且相邻类别的数据点可以分隔开，同时能够保证序的正确性。通过上面的推导和分析，WKFDOR算法的数学形式描述如下：
\begin{equation}
\label{wkfdor}
\left\{\begin{array}{rll} \min & J(\alpha,\rho)=\alpha^{T}\cdot \mathcal{N}_{\mu} \cdot \alpha-C\cdot \rho& \\
\\
s.t. & \alpha^{T}\cdot (\mathcal{M}_{k+1}-\mathcal{M}_{k})\geq\rho,{} \\
     & k = 1,2,\dots,K-1\\

\end{array}
\right.
\end{equation}
为了保证矩阵\(\mathcal{N}\)是正定的（ positive definite）， 令\(\mathcal{N}_{\mu}=\mathcal{N}+\mu I\)，其中
\(I\)是单位矩阵（ identity matrix）。

通过拉格朗日乘数法（Lagrange multipliers），可以将WKFDOR模型转化成一个有线性约束的二次规划（quadratic programming, QP）问题，而该问题能够很容易得到求解。需要指出的是，通过投影向量映射后的空间是一个一维空间。求解出\(\alpha\)之后（同时也可以得到最优投影向量\(w\)），相邻类别之间的阈值（在映射后的空间）根据下式计算：
\begin{equation}
\label{threshold_wkfdor}
b_{k}=\begin{cases}
\frac{\alpha^{T} \cdot (u_{k+1} \mathcal{M}_{k+1} + u_{k} \mathcal{M}_{k})}{u_{k+1}+u_{k}},\quad &k \in \{1,2,\dots,K-1\} \\
+\,\infty,\quad &k=K
\end{cases}
\end{equation}
可以看出，这些阈值是由有标签数据和无标签数据共同决定的。对于测试样例\(x\)，其投影结果是：
\begin{equation}
\label{projResult_wkfdor}
p(x)=w^{T} \cdot \phi(x)=\sum_{i=1}^{N} \alpha_{i} k(x_{i},x)
\end{equation}
最后，我们可以根据下面的规则预测 \(x\) 的序：
\begin{equation}
\label{rank_rule_wkfdor}
r(x)=\min \limits_{k \in \{1,2,\dots,K\}} \{k\mid p(x)-b_{k}<0\}
\end{equation}

不同于KDLOR算法，WKFDOR算法不仅使用了有标签数据，同时还利用了无标签数据来估计类均值和类内协方差矩阵。我们通过计算隶属度（\autoref{alg_mem}）和加权策略来使用这些无标签数据。由于在实际应用中通常存在大量的无标签数据，所以合理利用它们能够更加准确地估计潜在的数据分布。因此，我们提出的WKFDOR算法能够在半监督序回归场景下获得一个更好的性能。下一节中，我们将会通过实验来验证算法的有效性。

\section{实验验证}
为了说明我们提出的WKFDOR算法能够通过加权策略有效地使用无标签数据，我们设计了两组实验。首先，我们在人工合成的数据集上比较WKFDOR算法和KDLOR算法，并对结果可视化。其次，我们在10组真实的有序数据集上评估WKFDOR算法和KDLOR算法的性能。

\subsection{实验设置}
\label{wkfdor_expSet}
本实验采用下面的高斯核函数来做非线性映射：
\begin{equation}
\label{Gaussian_kernel}
k(x,y)=\exp(-\frac{\parallel x-y \parallel^{2}}{2\sigma^{2}})
\end{equation}
KDLOR算法的超参数\(\mu\)、\(C\)、 \(\sigma\)，以及\textit{mConsistency}算法的超参数\(\alpha\)、\(\varepsilon\) 通过五折交叉验证（5-fold cross validation）来确定。在这里，WKFDOR算法的\(\mu\)、\(C\)、 \(\sigma\)采用和KDLOR算法相同的值。使用相同的超参值来比较这两个算法看上去是公平的，实际上WKFDOR算法可能存在更优的参数设置。
% 数据预处理：对于离散属性（categorical）通过0-1编码来处理，使用z-score归一化数据（归一化成(0,1)高斯分布）

\subsection{合成数据集}
为了可视化和便于理解算法的学习结果，我们在这一小节中生成了一个二维数据集。该数据集共有4个类别，每个类别从一个二元高斯分布中生成50个样例。按照序从小到大，将这4个高斯分布的中心依次设置为(10,10)、(30,20)、 (20,10) 和(40,20)，并将它们的标准差都设置为(2,2)。对每一个类别，随机抽取25个样例作为测试数据，剩下的25个样例作为训练数据。再从训练数据中随机抽取5个作为有标签数据，剩下的20个作为无标签数据。数据的分布情况见\autoref{syn_labeled}，可以看出这是一个非线性的序回归问题。

在该实验中，我们在合成数据集上比较WKFDOR算法和KDLOR算法，其中无标签数据对于每个类别的权重通过\textit{mConsistency}算法计算得到。\autoref{syn_data}展示了训练数据的分布情况和算法的投影结果。\autoref{syn_labeled}画出了只用有标签数据估计的类别中心，可以看出它们和用整个数据集计算得到的类中心相距甚远，这是因为有标签数据量很少。而\autoref{syn_weightedMeans}画出的加权中心更加靠近于用整个数据集估计的类中心，这说明了我们提出的加权策略以及 \textit{mConsistency}算法是行之有效的。\autoref{kdlor_proj}和\autoref{wkfdor_proj}分别展示了KDLOR算法和WKFDOR算法得到的投影结果。对于KDLOR算法，有标签数据的投影可以按照序很好地分开，而测试数据在投影后的空间里相邻类别之间有严重的重叠，即对测试数据的序预测不准确。相反，对于WKFDOR算法，有标签数据和测试数据的投影都能够按照序很好地分开，尤其对测试数据的预测结果有显著的提升。合成数据集的实验结果说明，我们提出的WKFDOR算法能够有效地利用无标签数据从而提升处理序回归问题的性能。

\begin{figure*}[!htb]
%\centering
{\subfloat[有标签数据在原始空间的分布情况]{\includegraphics[width=3in]{figures/syn_labeled3}%
\label{syn_labeled}}
\hfil
\subfloat[加权策略估计的类中心情况]{\includegraphics[width=3in]{figures/syn_weightedMeans3}%
\label{syn_weightedMeans}}
\hfil
\subfloat[KDLOR算法的投影结果]{\includegraphics[width=3in]{figures/kdlor_proj3}%
\label{kdlor_proj}}
\hfil
\subfloat[WKFDOR算法的投影结果]{\includegraphics[width=3in]{figures/wkfdor_proj3}%
\label{wkfdor_proj}}}
\caption{合成数据集的数据分布情况和投影结果。$x^{1d}$ 和 $x^{2d}$ 分别对应样例的两个属性；$p^{l}(x)$是KDLOR算法的投影函数，其中$\alpha_{i}^{l}$是第$i$个有标签样例的系数。}
\label{syn_data}
\end{figure*}

\subsection{真实数据集}
\label{wkfdor_realData}

\begin{table}[!htbp]
\caption{真实数据集}
\label{table_realSets}
\centering
\begin{tabular}{l|cccc}
\toprule
Datasets & \# Attributes & \# Ranks & \# Training & \# Testing\\
\midrule
TAE & 5 & 3 & 100 & 51\\
Thyroid-new & 5 & 3 & 130 & 85\\
Balance & 4 & 3 & 400 & 225\\
Car & 6 & 4 & 160 & 100\\
SWD & 10 & 4 & 600 & 400\\
LEV & 4 & 5 & 600 & 400\\
ESL & 4 & 9 & 300 & 188\\
ERA & 4 & 9 & 600 & 400\\
Connect-4 & 42 & 3 & 7000 & 3000\\
\bottomrule
\end{tabular}
\end{table}

% 可以详细描述一下数据集
在这一小节中，我们使用10个真实的有序数据集（见\autoref{table_realSets}）来比较WKFDOR算法和KDLOR算法。TAE、Thyroid-new、Balance、Car、Connect-4取自UCI机器学习库（UCI machine learning repository）\footnote{Car和Connect-4分别从原始数据集随机抽取一部分样例用于本实验。}\citep{Bache+Lichman:2013}。
%[M. Lichman, “UCI machine learning repository,” 2013. [Online]. Available: http://archive.ics.uci.edu/ml]。
SWD、LEV、ESL、ERA是真实的人类决策数据集，其中属性和标签都是有序值\citep{ben2006generating}。
%［A. Ben-David and L. Sterling, “Generating rules from examples of human multiattribute decision making should be simple,” Expert Systems with Applications, vol. 31, no. 2, pp. 390–396, 2006.］。

对于每个数据集，取一半的训练数据作为有标签数据，剩下的一半作为无标签数据。分别将WKFDOR算法和KDLOR算法在每个数据集上跑20遍，统计MAE和MZE的平均值和标准差。具体结果见\autoref{table_realResults20m}。

%记得加上统计测试的结果＊
\begin{table*}[htbp]
\caption{KDLOR和WKFDOR在真实数据集上的测试结果。该结果是由运行20次的均值和方差组成，包括MAE和MZE两个指标。每组数据集的最优均值用粗体来表示。用秩和检验（Wilcoxon rank-sum test）做统计测试，其中显著性水平设为0.05，表中显著优于KDLOR的结果用$*$号来标记。}
\label{table_realResults20m}
\centering
\begin{tabular}{l|cc|cc}
\toprule
 & \multicolumn {2}{c|}{MAE} & \multicolumn {2}{c}{MZE} \\
 \cmidrule {2-5}
Datasets & KDLOR & WKFDOR & KDLOR & WKFDOR\\
\midrule
TAE &  {\bf 0.5402$\pm$0.0746} &  0.5471$\pm$0.0648 & 0.5392$\pm$0.0743 & {\bf 0.5353$\pm$0.0630} \\
Thyroid-new & 0.2788$\pm$0.0717 &  {\bf 0.2182$\pm$0.0576} & 0.2741$\pm$0.0736 & {\bf 0.2100$\pm$0.0619} \\
Balance & 0.4829$\pm$0.0427 &  {\bf 0.3109$\pm$0.0244}$^{*}$ & 0.4829$\pm$0.0427 & {\bf 0.3107$\pm$0.0244}$^{*}$ \\
Car & 0.3170$\pm$0.0546 &  {\bf 0.2760$\pm$0.0672} & 0.3070$\pm$0.0517 & {\bf 0.2740$\pm$0.0657} \\
SWD & {\bf 0.5740$\pm$0.0403} &  0.5833$\pm$0.0155 & 0.5097$\pm$0.0341 & {\bf 0.5060$\pm$0.0099} \\
LEV & {\bf 0.6447$\pm$0.0311} &  0.6570$\pm$0.0454 & 0.5307$\pm$0.0235 & {\bf 0.5142$\pm$0.0300} \\
ESL & {\bf 0.4415$\pm$0.0428} &  0.4660$\pm$0.0501 & {\bf 0.4436$\pm$0.0370} & 0.4553$\pm$0.0459 \\
ERA & 1.7706$\pm$0.1889 &  {\bf 1.7005$\pm$0.1524} & 0.8015$\pm$0.0256 & {\bf 0.7891$\pm$0.0238} \\
Connect-4 & 0.4982$\pm$0.0087 & {\bf 0.4828$\pm$0.0112}$^{*}$ & 0.4505$\pm$0.0094 & {\bf 0.3752$\pm$0.0097}$^{*}$ \\
%Connect-4 & 0.498183$\pm$0.008699 & 0.482817$\pm$0.011204 & 0.450467$\pm$0.009402 & 0.375200$\pm$0.009743 \\
\bottomrule
\end{tabular}
\end{table*}

MAE指标中，WKFDOR算法在5个数据集上结果的均值要优于KDLOR算法，其中在两个数据集上（Balance和Connect-4）显著优于KDLOR算法。MZE指标中，WKFDOR算法在8个数据集上结果的均值优于KDLOR算法，并在Balance和Connect-4数据集上显著优于KDLOR算法。整体来看，WKFDOR算法在处理序回归问题时具有更优的性能，尤其是MZE指标。在真实的序回归问题中，有标签数据缺乏往往导致学习器训练不够，从而在遇到新的测试数据时不能正确地进行预测。WKFDOR算法通过加权策略利用无标签数据来更准确地估计类分布，从而获得更优的泛化性能。该实验中，WKFDOR算法得到的MZE结果几乎都优于KDLOR算法，而在一些数据集上的MAE结果没有体现出优越性，原因可能是\textit{mConsistency}算法对这些数据集中无标签数据属于每个类别的权重估计得不够准确。\textit{mConsistency}算法没有考虑数据的序关系，更偏向于以准确率为目标进行信息传播。例如，有一个真实序为3的点，它到序为1和4的类的距离相同，算法可能会得到该点以相同的置信度属于类别1和4。而在序回归问题中，如果预测该点属于类别4的置信度大于属于类别1的置信度将更有建设性，因为将其预测为4的代价通常会小于预测为1的代价（即MAE所追求的差值最小）。因此，我们将在后面的章节中提出改进的算法，使其在计算权重时考虑序信息并且估计得更加准确，从而提升性能。

\section{小结}













